{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ee58a435",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6e241067",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_channel_power(data):\n",
        "    return np.mean(np.abs(data)**2)\n",
        "\n",
        "def compute_autocorrelation(hp_ls):\n",
        "    if hp_ls.ndim == 2:\n",
        "        hp_ls = hp_ls.unsqueeze(0)  # add batch dimension\n",
        "    assert hp_ls.ndim == 3\n",
        "\n",
        "    batch_size = hp_ls.shape[0]\n",
        "\n",
        "    hp_ls = hp_ls.reshape(batch_size, -1)  # row-major flatten\n",
        "\n",
        "    hp_ls_autocorr = np.conj(hp_ls.T) @ hp_ls\n",
        "    return hp_ls_autocorr / batch_size\n",
        "\n",
        "def compute_cross_correlation(hp_ls, h_true):\n",
        "    if hp_ls.ndim == 2:\n",
        "        hp_ls = hp_ls.unsqueeze(0)  # add batch dimension\n",
        "    \n",
        "    if h_true.ndim == 2:\n",
        "        h_true = h_true.unsqueeze(0)  # add batch dimension\n",
        "    \n",
        "    batch_size = hp_ls.shape[0]\n",
        "\n",
        "    assert hp_ls.ndim == 3 and h_true.ndim == 3\n",
        "    assert hp_ls.shape[0] == h_true.shape[0]\n",
        "\n",
        "    hp_ls = hp_ls.reshape(batch_size, -1)  # row-major flatten\n",
        "    h_true = h_true.reshape(batch_size, -1)  # row-major flatten\n",
        "\n",
        "    hp_ls_h_true_cross_corr = np.conj(h_true.T) @ hp_ls\n",
        "    \n",
        "    return hp_ls_h_true_cross_corr/ batch_size\n",
        "    \n",
        "\n",
        "def lmmse_interpolation(hp_ls, sigma2, pilot_autocorr, channel_pilot_corr):\n",
        "    sigma2_eye = np.eye(pilot_autocorr.shape[0]) * sigma2\n",
        "    pilot_autocorr_inv = np.linalg.inv(pilot_autocorr + sigma2_eye)\n",
        "    h_true_hat = channel_pilot_corr @ pilot_autocorr_inv @ hp_ls\n",
        "    return h_true_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "716c2716",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TDLDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, data_path, *, file_size, normalization_stats=None,return_pilots_only=True, num_subcarriers=120,\n",
        "        num_symbols=14, SNRs=[0, 5, 10, 15, 20, 25, 30],\n",
        "        pilot_symbols=[2, 11], pilot_every_n=2):\n",
        "        \"\"\"\n",
        "        This class loads the data from the folder and returns a dataset of channels.\n",
        "\n",
        "        data_path: path to the folder containing the data\n",
        "        file_size: number of channels per file\n",
        "        return_pilots_only: if True, only the LS channel estimate at pilots are returned\n",
        "            if False, the LS channel estimate is returned as a sparse channel matrix with non-zero \n",
        "            values only at the pilot subcarriers and time instants.\n",
        "        num_subcarriers: number of subcarriers\n",
        "        num_symbols: number of OFDM symbols\n",
        "\n",
        "        SNRs: list of SNR values to randomly sample from when return LS estimates.\n",
        "            AWGN is added to simulate LS estimatation error\n",
        "        pilot_symbols: list of OFDM symbol indices where pilots are placed\n",
        "        pilot_every_n: number of subcarriers between pilot subcarriers\n",
        "        \"\"\"\n",
        "        \n",
        "        self.file_size = int(file_size)\n",
        "        self.normalization_stats = normalization_stats\n",
        "        self.return_pilots_only = return_pilots_only\n",
        "        self.num_subcarriers = num_subcarriers\n",
        "        self.num_symbols = num_symbols\n",
        "        self.SNRs = SNRs\n",
        "        self.pilot_symbols = pilot_symbols\n",
        "        self.pilot_every_n = pilot_every_n\n",
        "        self.noise_variance = self._get_noise_variance(SNRs)\n",
        "\n",
        "        self.file_list = list(Path(data_path).glob(\"*.npy\"))\n",
        "        self.stats = self._get_stats_per_file(self.file_list)\n",
        "        self.data = self._load_data_from_folder(self.file_list, self.normalization_stats)\n",
        "        self.pilot_mask = self._get_pilot_mask()\n",
        "\n",
        "        self.num_pilot_symbols = len(self.pilot_symbols)\n",
        "        self.num_pilot_subcarriers = int(self.pilot_mask.sum()) // self.num_pilot_symbols\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list) * self.file_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_idx = idx // self.file_size\n",
        "        sample_idx = idx % self.file_size\n",
        "        file_path = self.file_list[file_idx]\n",
        "        channels = self.data[file_path]\n",
        "        channel = channels[sample_idx].squeeze().T\n",
        "\n",
        "        SNR = random.choice(self.SNRs)\n",
        "        LS_channel_at_pilots = self._get_LS_estimate_at_pilots(channel, SNR)\n",
        "        stats = self.stats[file_path]\n",
        "        stats[\"SNR\"] = SNR\n",
        "\n",
        "        LS_channel_at_pilots_torch = torch.from_numpy(LS_channel_at_pilots).to(torch.complex64)\n",
        "        channel_torch = torch.from_numpy(channel).to(torch.complex64)\n",
        "        return LS_channel_at_pilots_torch, channel_torch, stats\n",
        "    \n",
        "    @staticmethod\n",
        "    def _load_data_from_folder(file_list, normalization_stats=None):\n",
        "        data = {}\n",
        "        for file_path in file_list:\n",
        "            file_data = np.load(file_path)\n",
        "            if normalization_stats is not None:\n",
        "                normalized_real = (file_data.real - normalization_stats[\"real_mean\"]) / normalization_stats[\"real_std\"]\n",
        "                normalized_imag = (file_data.imag - normalization_stats[\"imag_mean\"]) / normalization_stats[\"imag_std\"]\n",
        "                file_data = normalized_real + 1j * normalized_imag\n",
        "            data[file_path] = file_data\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_stats_per_file(file_list):\n",
        "        stats = {}\n",
        "\n",
        "        for file_path in file_list:\n",
        "            file_name = str(file_path.stem)\n",
        "            file_parts = file_name.split(\"_\")\n",
        "\n",
        "            if file_parts[0] == \"delay\":\n",
        "                delay_spread = int(file_parts[2])  # [delay, spread, y, doppler, x]\n",
        "                doppler_shift = int(file_parts[-1])\n",
        "            elif file_parts[0] == \"doppler\":\n",
        "                doppler_shift = int(file_parts[1])  # [doppler, x, delay, spread, y]\n",
        "                delay_spread = int(file_parts[-1])\n",
        "            else:\n",
        "                raise ValueError(f\"File {file_name} has unexpected format\")\n",
        "            \n",
        "            if file_path not in stats:\n",
        "                stats[file_path] = {\"doppler_shift\": doppler_shift, \"delay_spread\": delay_spread}\n",
        "            else:\n",
        "                raise ValueError(f\"File {file_path} already in stats, but should not be\")\n",
        "            \n",
        "        return stats\n",
        "    \n",
        "    def _get_noise_variance(self, SNRs):\n",
        "        noise_variances = []\n",
        "        for SNR in SNRs:\n",
        "            noise_variance = 1 / (10**(SNR / 10))\n",
        "            noise_variances.append(noise_variance)\n",
        "        return np.mean(np.array(noise_variances))\n",
        "    \n",
        "    def _get_LS_estimate_at_pilots(self, channel_matrix, SNR):\n",
        "        # unit symbol power and unit channel power --> rx noise var = LS error var\n",
        "        noise_std = np.sqrt(1 / (10**(SNR / 10)))\n",
        "        noise_real_imag = noise_std / np.sqrt(2)\n",
        "\n",
        "        if self.return_pilots_only:\n",
        "            pilot_mask_bool = self.pilot_mask.astype(bool)\n",
        "            channel_at_pilots = channel_matrix[pilot_mask_bool]\n",
        "            channel_at_pilots = channel_at_pilots.reshape(self.num_pilot_subcarriers, self.num_pilot_symbols)\n",
        "            noise_real = noise_real_imag * np.random.randn(self.num_pilot_subcarriers, self.num_pilot_symbols)\n",
        "            noise_imag = noise_real_imag * np.random.randn(self.num_pilot_subcarriers, self.num_pilot_symbols)\n",
        "            noise = noise_real + 1j * noise_imag\n",
        "        else:\n",
        "            channel_at_pilots = self.pilot_mask * channel_matrix\n",
        "            noise_real = noise_real_imag * np.random.randn(self.num_subcarriers, self.num_symbols)\n",
        "            noise_imag = noise_real_imag * np.random.randn(self.num_subcarriers, self.num_symbols)\n",
        "            noise = noise_real + 1j * noise_imag\n",
        "            noise = noise * self.pilot_mask\n",
        "        \n",
        "        channel_at_pilots_LS = channel_at_pilots + noise\n",
        "            \n",
        "        return channel_at_pilots_LS\n",
        "\n",
        "    def _get_pilot_mask(self):\n",
        "        pilot_mask = np.zeros((self.num_subcarriers, self.num_symbols))\n",
        "        pilot_mask_subcarrier_indices = np.arange(0, self.num_subcarriers, self.pilot_every_n)\n",
        "        pilot_mask[np.ix_(pilot_mask_subcarrier_indices, self.pilot_symbols)] = 1\n",
        "        return pilot_mask\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "42800441",
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_SET_PATH = \"/opt/shared/datasets/NeoRadiumTDLdataset/train/TDLA\"\n",
        "TEST_SNR_LEVELS = [100]\n",
        "LIMIT_CHANNELS = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7225caa2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 240 files in the dataset\n",
            "Average channel power: 1.0\n"
          ]
        }
      ],
      "source": [
        "file_list = list(Path(TRAIN_SET_PATH).glob(\"*.npy\"))\n",
        "\n",
        "print(\"There are\", len(file_list), \"files in the dataset\")\n",
        "\n",
        "channel_power_list = []\n",
        "for file in file_list:\n",
        "    channel_power_list.append(compute_channel_power(np.load(file).squeeze()))\n",
        "\n",
        "print(\"Average channel power:\", np.array(channel_power_list).mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e1285e72",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(Path(TRAIN_SET_PATH, \"metadata.yaml\"), \"r\") as f:\n",
        "    train_metadata = yaml.safe_load(f)\n",
        "\n",
        "large_dataset = TDLDataset(\n",
        "    TRAIN_SET_PATH, \n",
        "    file_size=train_metadata[\"config\"][\"num_channels_per_config\"],\n",
        "    SNRs=TEST_SNR_LEVELS,\n",
        "    return_pilots_only=True)\n",
        "\n",
        "small_dataset_indices = np.random.choice(len(large_dataset), size=LIMIT_CHANNELS, replace=False)\n",
        "\n",
        "small_dataset = Subset(large_dataset, small_dataset_indices)\n",
        "\n",
        "data_loader = DataLoader(small_dataset, batch_size=512, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a39fa3d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "size_hp_ls = large_dataset.num_pilot_subcarriers * large_dataset.num_pilot_symbols\n",
        "size_h_true = large_dataset.num_subcarriers * large_dataset.num_symbols\n",
        "pilot_autocorr = np.zeros((size_hp_ls, size_hp_ls), dtype=np.complex128)\n",
        "h_true_hp_ls_crosscorr = np.zeros((size_h_true, size_hp_ls), dtype=np.complex128)\n",
        "\n",
        "for batch in data_loader:\n",
        "    hp_ls, h_true, batch_stats = batch\n",
        "    hp_ls = hp_ls.numpy()\n",
        "    h_true = h_true.numpy()\n",
        "    pilot_autocorr += compute_autocorrelation(hp_ls)\n",
        "    h_true_hp_ls_crosscorr += compute_cross_correlation(hp_ls, h_true)\n",
        "\n",
        "pilot_autocorr /= len(data_loader)\n",
        "h_true_hp_ls_crosscorr /= len(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "854e86ec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R_hphp shape (120, 120)\n",
            "R_hhp shape (1680, 120)\n",
            "Channel power: 0.9770109874506792 when SNRs are [100]\n",
            "Noise Power: 1e-10 when SNRs are [100]\n"
          ]
        }
      ],
      "source": [
        "print(\"R_hphp shape\", pilot_autocorr.shape)\n",
        "print(\"R_hhp shape\", h_true_hp_ls_crosscorr.shape)\n",
        "\n",
        "print(\"Channel power:\", np.mean(np.diag(pilot_autocorr).real), \"when SNRs are\", TEST_SNR_LEVELS)\n",
        "print(\"Noise Power:\", large_dataset.noise_variance, \"when SNRs are\", TEST_SNR_LEVELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0e36d376",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 677.38it/s]\n"
          ]
        }
      ],
      "source": [
        "errors = []\n",
        "for channel in tqdm(small_dataset):\n",
        "    hp_ls, h_true, batch_stats = channel\n",
        "    hp_ls = hp_ls.numpy()\n",
        "    h_true = h_true.numpy()\n",
        "\n",
        "    hp_ls = hp_ls.flatten()  # row-major flatten\n",
        "    \n",
        "    h_true_hat = lmmse_interpolation(hp_ls, large_dataset.noise_variance, pilot_autocorr, h_true_hp_ls_crosscorr)\n",
        "    h_true_hat = h_true_hat.reshape(large_dataset.num_subcarriers, large_dataset.num_symbols)\n",
        "\n",
        "    error = np.mean(np.abs(h_true - h_true_hat)**2)\n",
        "    errors.append(error) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "31bf6d03",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "416.9051680059532"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(errors).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ca09036a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1229649040793855"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(errors).min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d65d146b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11066.227087509948"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(errors).max()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lwm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
